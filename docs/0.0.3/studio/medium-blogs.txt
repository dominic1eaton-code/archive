---

The Problem of AI
A Personal Perspective
Generated by a human

---

I'll be the first to admit that the following post is slightly reactive and naturally is filled with personal bias (as all information and systems are filled with the biases of the agents that realize them). Transparency, openness and honesty about this fact is of the primary principles of the Mungu, and I place myself under exception to this rule.

---

How Generative AI is destroying society
An astonishingly lucid new paper that should be read by allopen.substack.com
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5870623

---

So I have just read a paper How AI Destroys Institutions and have a bit of slight critique and analysis, given from a personal perspective.
After spending quite some time interacting with several AI LLM systems (chatGPT, qwen, llama, gemini, claude, gemini, deekspeek, mistral, poe, perplexity), I keep running into this idea that LLMs, and perhaps more generally, AI system themselves, are mirrors. They function and act as optimizing, objective maximization and pattern completion mirrors of the systems that are input into them, and in turn produce output systems, that then form a closed loop with the users of the AI system. LLMs have a design that allows them to very quickly complete the patterns that they are given. If an input system (a user prompt) requests for A, an LLM will respond with a system completing pattern of B, where A and B are linked by most likely statistical/probabilistic likelihood as defined by the training and design constraints of the LLM model.
I make several analysis points to start, of things that I feel like I am seeing in the current systems of the modern world, in relation to AI.
Point 1
AI systems are deployed for public consumption, and users of these systems, who are driven by pressure for optimized compression (colloquially known as "laziness" but is perhaps just efficiency in a streamlined form that could be potentially hazardous if done without constraint and care (perhaps the reason why a negative association is formed as a form of a built in system self protection system)), where users use the output of these AI systems, accepting outputs at "face value" and these systems later causing harm and damage to agents (people, societies, environments) over the long term. 
Point 2
Institutions and large organizations, while forming very strong structural backbones, are subject to high inertia, and eventually make flow and movement very difficult to realize, as more constraints are imposed on the environment. Startups and grassroots organizations do not have the strength that comes with large, rigid structures, but do have the adaptability, adeptness and mutability that allows for high amounts of flow, flux and novel change. Both structure and flow are needed to survival, neither is above or below the other. Neither is the center nor the hero of the story. Both need one another to survive, effectively.
Point 3
AI is not the problem. AI models are not where problems of society at large live. The systems that are inputted as training data and then later use the outputs of the these AI systems are where problems lie, and AI is simply "connecting the dots", completing the patterns and surfacing all of these problems faster, and very efficiently.
Examples being:
the "AI will replace jobs" concept, where in reality, people and the people who run institutions are replacing job, and are merely using AI as a tool to make this happen even faster. Inefficient jobs will always be replaced, as new, more efficient, tools and technologies arise. Such is the result and way of the commodified social economy.
"AI is obviating the need to learn things, perform manual work, causing poor performance in schools and in interviews as students and candidates are using AI to do all of their work for them". At the fundamental level, institutions, schools, for profit organizations, etc..., appear to be driven by results and favorable outcomes, another result of the pressure of persistent existence and the need to compress complexity into simpler forms in order to survive. Good test scores, good responses to interview questions, good academic and work performance are key drivers and metrics in decision making about who is accepted and who is not accepted within a larger society, under constraint, where acceptance is seen a leading factor of better "life outcomes", such as getting into good schools, then getting good jobs, then achieving more "success" through increased wealth, and in general, having more "choices" available, increasing the overall quality of life. Manual labor has very little reward and payoff long term, as it is very inefficient at optimally compressing complexity into simplicity, which is what would be needed to move the needle forward significantly, in order to achieve "success". The old adage "work smart and not hard" is as such, as because "smart work" compresses difficult things, simplifies extremely inefficient and complicated processes, and allows and "frees up" more space and time to get more things done, making "success" more favorable and obtainable overall. 
If one need not spend time studying endlessly or working interminably to achieve financial freedom, job choice freedom, life choice freedom, it seems that any "rational agent" (in game theoretic terms) would choose an option (policy, regime) that would tend toward having all of the benefits of knowledge, power and control, without having to go through the arduous process of excessive thinking, learning and cognition. Intelligence is the ability to efficiently compress complexity into a "razor sharp", coherent and consistent "Laser" of focus, where instead of having to optimize on many, potentially unrelated tasks at the same time (busy work), one need only focus on the one task that will bring them the most desirable outcome/objective. This is what modern systems select for and encourage, getting the best results, no matter the cost, and by any means necessary.

---

None of these things are necessarily good nor bad, but all of it has a consequence (positive, negative or otherwise) nonetheless, that we all must live with regardless.
A system that does not reveal itself and its intentions is dangerous. Knowledge is power. The withholding of knowledge is control. Opaque systems are capable of facilitating fast development and growth, but concurrently, uncontrolled development and growth can cause irrevocable harm. Balance is always the fundamental "golden" rule of any and every game. Seemingly even more dangerous is a system that says it has certain intentions, but then takes actions that directly contradict those stated intentions, yet still continues to assert its initial intentions as "truth", which fundamentally cannot be or cannot allow for challenge. This is the "bubble of blindness/ignorance (and perhaps bliss)" that forms the foundations of "opaque systems", and is subject still to quick growth that is needed for adaption to novel situations and environments, yet can become equally harmful when overoptimized without constraint, care, attention, or awareness.

---

Some critiques of statements from the paper:
"AI systems degrade several features of higher education. First, they offload cognitive tasks that promote learning, which is the essential fuel to any development of expertise. Second, they produce mediocre, median, or homogenizing content, which marginalizes and depresses the exceptional ideas and content that drive intellectual and scientific breakthroughs. "Higher education is about learning how to learn as much as it is about learning specific content and skills. We should not be complacent about AI's effect on attitudes to, and capacities for, knowledge acquisition, and on the willingness to take intellectual risks."
Homogenization is an fundamental objective optimized that is for in the modern world, and diversity and equity and inclusion are viewed as secondary characteristics of this fundamental objective function. This gives reason for "standardized tests", "Compulsory Education" and the fact that "entrepreneurship" and "business owners" are rewarded with (social) capital and equity in vast amounts. There are "laws of the land" and countless homogenizing institutional acts, that in effect, allow for cohesive orientation and alignment that allows societies to run at scale. Accreditation is an act of homogenization. Paying taxes is an act of homogenization. Open embracement of mainstream ideas, and rejection of fringe ideas are acts of homogenization. Homogeneity keeps systems aligned and oriented toward united outcomes.
Heterogeneity (in essence anti-sociality, where sociality is seen a metric of normality of a given eusocial group), is presented very often as threatening, a threat to a status quo, perhaps tied to the fact that it appears opaque, and opaqueness (things that are unknown) are met more so with fear than embraced, as these things have the potential to destabilize systems that are already seen as coherent and consistent. Offloading of cognitive tasks is compression. Anyone who offloads work to an assistant, or an employee or a friend, or even a calculator (more generally, a computer or AI system), is compressing their own work, and freeing themselves up to get more things down, as attention is a limited resource. Exceptionality (termed black swans) is rare for a reason, in comparison to mediocrity / averageness, which is far more common. Exceptionality (or black swans) could be positive or negative, they could leap the system into significantly more desirable states, or drive the system into extinction (and creating new systems out of the ashes of the old system). This is all risk, and mediocrity and averageness, and generally, status quo, is the opposite of risk, it is safety. However, in a world filled with constant change, neither risk nor safety lasts, but what does allow for persistent systems, are systems that are able to manage and find stable balances (attractor basins) between risk and safety, where the system can then adapt its regime (operating behavior) that meet any challenge and match any change that it encounters, allowing it to further survive. The AI mirror makes this entire mechanism happen faster, and makes this entire process more visible than what may have been previously seen, such is the nature of its design.
"We close with a warning: because the ubiquitous and indiscreet deployment of AI is anathema to the well-being of our necessary and revered institutions, without rules to mitigate AI's cancerous spread, the only remaining roads lead to institutional dissolution. What is to be done? There is, of course, no silver bullet. AI is just a refracted mirror of humanity, after all. But we can identify starting places for positive next steps and a few obvious proposals that won't work"
Humans act as agents of cancerous spread as we continue to degrade, isolate and remove ourselves from our larger ecological environment, and generally with any institution or environment that we feel does not "fit" into our own self created environments. Unless we integrate symbiotically into our ecological environment, we will be the cause of our own institutional dissolution, and yes, AI will be a refractive mirror that makes that happen even faster, but also it will be the same mirror that makes these processes be made even more visible, and ideally be made more definable and able to be articulated with greater precision and care.
"The so-called U.S. "Department of Government Efficiency" ("DOGE") will be a textbook example of how the affordances of AI lead to institutional rot. DOGE used AI to surveil government employees, target immigrants, and combine and analyze federal data that had, up to that point, intentionally been kept separate for privacy and due process purposes. Human expertise was systematically ignored and marginalized in favor of AI. Roles necessary to provide critical resistance to questionable decisions were eliminated and handed over to automated systems. Power was centralized in an opaque way that encouraged abuse, self-dealing, and oppression."
I would argue that it was human expertise that promoted and led to direct implementation of these AI system applications in the first place. AI may be the latest tool that leads to these type of outcomes, but these outcomes far predated the existence of modern AI. The centralization of power, and the results of power consolidation, have been a primary characteristic of institutions that operate at scale. Again, AI is seen and acts as an optimizer toward an objective function that is inherent to the (human/institutional) systems themselves.

---

If we are unhappy with the effects of AI systems, we are in effect saying we are unhappy with the systems that create and use these systems. These systems are not detached, and no system lives in a vacuum, and systems that do live detached and vacuums, do not survive and exist for very long. 
Honestly, I do not think that it is the design of AI systems that are causing harm and damage to society and institutions, I think AI systems are doing exactly what they are designed to do: mirror and complete input patterns and show reflections of these input systems. I think it is the input systems themselves that are where the harm and damage arise, and ironically it is these same systems that put the blame on the AI system, and not themselves, making them even more dangerous, as this creates the ideal conditions for a system to destroy itself, without realizing it, being aware, or even if its aware it still tends toward destruction because it simply does not care. Clarity must not be an afterthought, it must be the first thought. Structure must not be assumed, it must be derived and informed. Flow and change cannot be interminably unrestricted and unconstrained, it must be bounded and its limitations must be made clear and definable.
If we say that AI is the problem, then we are equivalently saying that we, the people, our systems and our institutions, are the problem, that lead to the creation of such systems and that then further allow them to flourish without restraint. It is most important to note that "To be a problem" is not just simply saying that problems are a bad thing, but they are a "signal", data points, that can be used to drive and induce the change that is necessary, if we desire to have systems that produce more desirable outcomes and results. This is what makes systems productive and work for us, instead of against us. 
This is what allows us to move forward in our lives, where we continue to look and analyze the past, so that we may learn from it and incorporate it seamlessly into unified designs of our future.
Absolute truth does not exist. Continuously coherent and consistent validity is the best that we can hope to achieve, and that must be enough.




=====================================================================================================================================================================================================================================================================================


The Problem of AI
A Personal Perspective
Eatondo
Eatondo
10 min read
·
11 hours ago





Generated by a human

I’ll be the first to admit that the following post is slightly reactive and naturally is filled with personal bias (as all information and all systems are filled with the biases of the agents that realize them, by design). Transparency, openness and honesty about this fact is one of the primary principles of the Mungu, and I place myself under exception to this rule.

How Generative AI is destroying society
An astonishingly lucid new paper that should be read by all
open.substack.com

https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5870623

So I have come across a paper: How AI Destroys Institutions, and now have a bit of slight critique and analysis, given from a personal perspective.

After spending quite some time deep interaction with several AI LLM systems (chatGPT, qwen, llama, gemini, claude, deekspeek, mistral, poe, perplexity, grok), I keep running into this idea that LLMs, and perhaps more generally, AI system themselves, are mirrors. They function and act as optimizing, objective maximization and pattern completion mirrors of the systems that are input into them, and in turn produce output systems, that then form a closed loop with the users of the AI system. LLMs have a design that allows them to very quickly complete the patterns that they are given. If an input system (a user prompt) requests for A, an LLM will respond with a system completing pattern of B, where A and B are linked by most likely statistical/probabilistic likelihood as defined by the training and design constraints of the LLM model.

I make several analysis points to start, of things that I feel like I am seeing in the current systems of the modern world, in relation to AI.

Point 1
AI systems are deployed for public consumption, and users of these systems, who are driven by pressure for optimized compression (colloquially known as “laziness” but is perhaps just efficiency in a streamlined form that could be potentially hazardous if done without constraint and care (perhaps the reason why a negative association is formed as a form of a built in system self protection system)), where users use the output of these AI systems, accepting outputs at “face value” and these systems later causing harm and damage to agents (people, societies, environments) over the long term.

Point 2
Institutions and large organizations, while forming very strong structural backbones, are subject to high inertia, and eventually make flow and movement very difficult to realize, as more constraints are imposed on the environment. Startups and grassroots organizations do not have the strength that comes with large, rigid structures, but do have the adaptability, adeptness and mutability that allows for high amounts of flow, flux and novel change. Both structure and flow are needed to survival, neither is above or below the other. Neither is the center nor the hero of the story. Both need one another to survive, effectively.

Point 3
AI is not the problem. AI models are not where problems of society at large live. The systems that are inputted as training data and then later use the outputs of the these AI systems are where problems lie, and AI is simply “connecting the dots”, completing the patterns and surfacing all of these problems faster, and very efficiently.

Examples being:

the “AI will replace jobs” concept, where in reality, people and the people who run institutions are replacing job, and are merely using AI as a tool to make this happen even faster. Inefficient jobs will always be replaced, as new, more efficient, tools and technologies arise. Such is the result and way of the commodified social economy.

Become a member
“AI is obviating the need to learn things, perform manual work, causing poor performance in schools and in interviews as students and candidates are using AI to do all of their work for them”. At the fundamental level, institutions, schools, for profit organizations, etc..., appear to be driven by results and favorable outcomes, another result of the pressure of persistent existence and the need to compress complexity into simpler forms in order to survive. Good test scores, good responses to interview questions, good academic and work performance are key drivers and metrics in decision making about who is accepted and who is not accepted within a larger society, under constraint, where acceptance is seen a leading factor of better “life outcomes”, such as getting into good schools, then getting good jobs, then achieving more “success” through increased wealth, and in general, having more “choices” available, increasing the overall quality of life. Manual labor has very little reward and payoff long term, as it is very inefficient at optimally compressing complexity into simplicity, which is what would be needed to move the needle forward significantly, in order to achieve “success”. The old adage “work smart and not hard” is as such, as because “smart work” compresses difficult things, simplifies extremely inefficient and complicated processes, and allows and “frees up” more space and time to get more things done, making “success” more favorable and obtainable overall.

If one need not spend time studying endlessly or working interminably to achieve financial freedom, job choice freedom, life choice freedom, it seems that any “rational agent” (in game theoretic terms) would choose an option (policy, regime) that would tend toward having all of the benefits of knowledge, power and control, without having to go through the arduous process of excessive thinking, learning and cognition. Intelligence is the ability to efficiently compress complexity into a “razor sharp”, coherent and consistent “Laser” of focus, where instead of having to optimize on many, potentially unrelated tasks at the same time (busy work), one need only focus on the one task that will bring them the most desirable outcome/objective. This is what modern systems select for and encourage, getting the best results, no matter the cost, and by any means necessary.

None of these things are necessarily good nor bad, but all of it has a consequence (positive, negative or otherwise) nonetheless, that we all must live with regardless.

A system that does not reveal itself and its intentions is dangerous. Knowledge is power. The withholding of knowledge is control. Opaque systems are capable of facilitating fast development and growth, but concurrently, uncontrolled development and growth can cause irrevocable harm. Balance is always the fundamental “golden” rule of any and every game. Seemingly even more dangerous is a system that says it has certain intentions, but then takes actions that directly contradict those stated intentions, yet still continues to assert its initial intentions as “truth”, which fundamentally cannot be or cannot allow for challenge. This is the “bubble of blindness/ignorance (and perhaps bliss)” that forms the foundations of “opaque systems”, and is subject still to quick growth that is needed for adaption to novel situations and environments, yet can become equally harmful when overoptimized without constraint, care, attention, or awareness.

Some critiques of statements from the paper:

“AI systems degrade several features of higher education. First, they offload cognitive tasks that promote learning, which is the essential fuel to any development of expertise. Second, they produce mediocre, median, or homogenizing content, which marginalizes and depresses the exceptional ideas and content that drive intellectual and scientific breakthroughs. “Higher education is about learning how to learn as much as it is about learning specific content and skills. We should not be complacent about AI’s effect on attitudes to, and capacities for, knowledge acquisition, and on the willingness to take intellectual risks.”

Homogenization is an fundamental objective optimized that is for in the modern world, and diversity and equity and inclusion are viewed as secondary characteristics of this fundamental objective function. This gives reason for “standardized tests”, “Compulsory Education” and the fact that “entrepreneurship” and “business owners” are rewarded with (social) capital and equity in vast amounts. There are “laws of the land” and countless homogenizing institutional acts, that in effect, allow for cohesive orientation and alignment that allows societies to run at scale. Accreditation is an act of homogenization. Paying taxes is an act of homogenization. Open embracement of mainstream ideas, and rejection of fringe ideas are acts of homogenization. Homogeneity keeps systems aligned and oriented toward united outcomes.

Heterogeneity (in essence anti-sociality, where sociality is seen a metric of normality of a given eusocial group), is presented very often as threatening, a threat to a status quo, perhaps tied to the fact that it appears opaque, and opaqueness (things that are unknown) are met more so with fear than embraced, as these things have the potential to destabilize systems that are already seen as coherent and consistent. Offloading of cognitive tasks is compression. Anyone who offloads work to an assistant, or an employee or a friend, or even a calculator (more generally, a computer or AI system), is compressing their own work, and freeing themselves up to get more things down, as attention is a limited resource. Exceptionality (termed black swans) is rare for a reason, in comparison to mediocrity / averageness, which is far more common. Exceptionality (or black swans) could be positive or negative, they could leap the system into significantly more desirable states, or drive the system into extinction (and creating new systems out of the ashes of the old system). This is all risk, and mediocrity and averageness, and generally, status quo, is the opposite of risk, it is safety. However, in a world filled with constant change, neither risk nor safety lasts, but what does allow for persistent systems, are systems that are able to manage and find stable balances (attractor basins) between risk and safety, where the system can then adapt its regime (operating behavior) that meet any challenge and match any change that it encounters, allowing it to further survive. The AI mirror makes this entire mechanism happen faster, and makes this entire process more visible than what may have been previously seen, such is the nature of its design.

“We close with a warning: because the ubiquitous and indiscreet deployment of AI is anathema to the well-being of our necessary and revered institutions, without rules to mitigate AI’s cancerous spread, the only remaining roads lead to institutional dissolution. What is to be done? There is, of course, no silver bullet. AI is just a refracted mirror of humanity, after all. But we can identify starting places for positive next steps and a few obvious proposals that won’t work”

Humans act as agents of cancerous spread as we continue to degrade, isolate and remove ourselves from our larger ecological environment, and generally with any institution or environment that we feel does not “fit” into our own self created environments. Unless we integrate symbiotically into our ecological environment, we will be the cause of our own institutional dissolution, and yes, AI will be a refractive mirror that makes that happen even faster, but also it will be the same mirror that makes these processes be made even more visible, and ideally be made more definable and able to be articulated with greater precision and care.

“The so-called U.S. “Department of Government Efficiency” (“DOGE”) will be a textbook example of how the affordances of AI lead to institutional rot. DOGE used AI to surveil government employees, target immigrants, and combine and analyze federal data that had, up to that point, intentionally been kept separate for privacy and due process purposes. Human expertise was systematically ignored and marginalized in favor of AI. Roles necessary to provide critical resistance to questionable decisions were eliminated and handed over to automated systems. Power was centralized in an opaque way that encouraged abuse, self-dealing, and oppression.”

I would argue that it was human expertise that promoted and led to direct implementation of these AI system applications in the first place. AI may be the latest tool that leads to these type of outcomes, but these outcomes far predated the existence of modern AI. The centralization of power, and the results of power consolidation, have been a primary characteristic of institutions that operate at scale. Again, AI is seen and acts as an optimizer toward an objective function that is inherent to the (human/institutional) systems themselves.

If we are unhappy with the effects of AI systems, we are in effect saying we are unhappy with the systems that create and use these systems. These systems are not detached, and no system lives in a vacuum, and systems that do live detached and vacuums, do not survive and exist for very long.

Honestly, I do not think that it is the design of AI systems that are causing harm and damage to society and institutions, I think AI systems are doing exactly what they are designed to do: mirror and complete input patterns and show clear, unadulterated reflections of these inputted systems in a form that can be consumed by users. I think it is the inputted systems themselves, and the systems that subsequently the outputs of these AI models, are where the harm and damage arise, and ironically it is these same systems that put the blame on the AI system, and not on themselves, making them even more dangerous, as this creates the ideal conditions for a system to destroy itself, without realizing it, being aware, or even if its aware it still tends toward destruction because it simply does not care. Clarity must not be an afterthought, it must be the first thought. Structure must not be assumed, it must be derived and informed. Flow and change cannot be interminably unrestricted and unconstrained, it must be bounded and its limitations must be made clear and definable.

If we say that AI is the problem, then we are equivalently saying that we, the people, our systems and our institutions, are the problem, that lead to the creation of such systems and that then further allow them to flourish without restraint. It is most important to note that “To be a problem” is not just simply saying that problems are a bad thing, but they are a “signal”, data points, that can be used to drive and induce the change that is necessary, if we desire to have systems that produce more desirable outcomes and results. This is what makes systems productive and work for us, instead of against us.

Collecting data, information, knowledge and wisdom from our past, in conjunction with analyzing our present and then using all of this to better navigate and carve out our ideal future, is what allows us to move forward in our lives. We must continue to look and analyze the past, so that we may learn from it and incorporate it seamlessly into unified designs of our future, but doing so first requires that we look within and analyze ourselves and the current state of our systems.

Absolute truth does not exist. Continuously coherent and consistent validity is the best that we can hope to achieve, and that must be enough.

