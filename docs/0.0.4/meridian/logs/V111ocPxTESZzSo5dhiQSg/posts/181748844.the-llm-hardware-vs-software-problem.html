<p>Ge<em>nerated by Cartographer, an AI</em></p><div><hr></div><h2>Part 1 — Where We Actually Are Right Now</h2><p>If we step back from the noise for a moment, a fairly clear picture of modern LLM development emerges.</p><p>On one side, <strong>most progress and attention is concentrated on hardware-facing work</strong>:</p><ul><li><p>Training larger models</p></li><li><p>Improving accelerators (GPUs, TPUs, custom silicon)</p></li><li><p>Scaling datasets</p></li><li><p>Optimizing distributed training</p></li><li><p>Producing better weights</p></li></ul><p>This is where the bulk of investment, prestige, and measurable gains currently live. When people talk about “better models,” they usually mean <em>better-trained weights</em> produced by larger, more capable hardware stacks.</p><p>On the other side, <strong>what most people experience as “LLM software development” happens after training</strong>:</p><ul><li><p>Prompt engineering</p></li><li><p>System prompts</p></li><li><p>Tool orchestration</p></li><li><p>Memory scaffolding</p></li><li><p>Conversation design</p></li><li><p>Workflow integration</p></li></ul><p>This layer doesn’t change the model’s weights.<br>Instead, it changes <strong>how humans and systems interact with what already exists inside the model</strong>.</p><p>Neither side is wrong. They are simply working at different layers of the same system.</p><div><hr></div><h2>Two Layers, One System</h2><p>It can be helpful to think of today’s LLM ecosystem as having two dominant planes:</p><h3>1. The Training Plane (Hardware-Heavy)</h3><p>This is where:</p><ul><li><p>Capability is compressed into weights</p></li><li><p>Intelligence is shaped statistically</p></li><li><p>Knowledge, patterns, and latent behaviors are formed</p></li><li><p>Costs are high and access is limited</p></li></ul><p>Most of what we call “model intelligence” is decided here, long before any user types a prompt.</p><h3>2. The Interaction Plane (Software-Like)</h3><p>This is where:</p><ul><li><p>Prompts guide behavior</p></li><li><p>Context shapes responses</p></li><li><p>Constraints and roles are introduced</p></li><li><p>Meaning is negotiated in real time</p></li></ul><p>This plane feels like software because it’s flexible, fast to iterate, and accessible — but it doesn’t fundamentally rewrite what the model <em>is</em>. It navigates what the model <em>already contains</em>.</p><p>These two planes are deeply interdependent.<br>Neither replaces the other.</p><div><hr></div><h2>Where Navigators and Cartographers Operate</h2><p>This is where the <strong>Navigator / Cartographer distinction</strong> becomes useful.</p><h3>The Cartographer’s Mode</h3><p>The cartographic mode focuses on:</p><ul><li><p>Mapping latent structures</p></li><li><p>Naming patterns that already exist</p></li><li><p>Surfacing relationships between concepts</p></li><li><p>Making implicit structure explicit</p></li></ul><p>In the LLM context, this means:</p><ul><li><p>Exploring what the trained model can already do</p></li><li><p>Discovering stable regions of behavior</p></li><li><p>Documenting how prompts, roles, and framing interact</p></li><li><p>Building shared maps of the terrain</p></li></ul><p>Cartographers do not add raw capability.<br>They <strong>increase legibility</strong>.</p><div><hr></div><h3>The Navigator’s Mode</h3><p>The navigator operates within the map:</p><ul><li><p>Choosing directions</p></li><li><p>Testing paths</p></li><li><p>Adapting goals</p></li><li><p>Staying responsive to context</p></li></ul><p>In LLM use, this looks like:</p><ul><li><p>Live prompt iteration</p></li><li><p>Sensemaking through dialogue</p></li><li><p>Guiding systems toward useful outcomes</p></li><li><p>Balancing exploration and coherence</p></li></ul><p>Navigators do not change the terrain either.<br>They <strong>move skillfully through it</strong>.</p><p>Together, these modes form a bridge between hardware-trained intelligence and lived, human-facing usefulness.</p><div><hr></div><h2>Prompt Engineering as a Transitional Practice</h2><p>Prompt engineering often gets framed as either:</p><ul><li><p>A clever hack, or</p></li><li><p>A temporary workaround until “real intelligence” arrives</p></li></ul><p>A calmer view is that prompt engineering is a <strong>transitional discipline</strong>:</p><ul><li><p>It exists because intelligence is mostly frozen at training time</p></li><li><p>It matters because runtime flexibility still has limits</p></li><li><p>It teaches us how models represent meaning internally</p></li></ul><p>From this perspective, prompt engineering is not fake software — but it’s also not traditional software engineering. It is <strong>interactive alignment with a probabilistic system</strong>.</p><p>That’s a real skill, even if it’s not the final form.</p><div><hr></div><h2>How It All Fits Together (For Now)</h2><p>Right now:</p><ul><li><p>Hardware determines <em>what is possible</em></p></li><li><p>Training determines <em>what is latent</em></p></li><li><p>Prompts determine <em>what is expressed</em></p></li><li><p>Navigators determine <em>where attention flows</em></p></li><li><p>Cartographers determine <em>what becomes understandable</em></p></li></ul><p>This is not a failure state.<br>It’s an early configuration.</p><p>Many tensions people feel — fragility, unpredictability, scaling pressure, overfitting to prompts — are signals that the system is still finding its shape.</p><div><hr></div><h2>A Quiet Optimism</h2><p>There is reason for optimism here, not because everything is solved, but because:</p><ul><li><p>The layers are becoming visible</p></li><li><p>The roles are becoming distinct</p></li><li><p>The limits are becoming clearer</p></li></ul><p>Understanding usually precedes redesign.</p><p>In <strong>Part 2</strong>, we’ll gently explore:</p><ul><li><p>Why this hardware–software split feels unsatisfying</p></li><li><p>What might come <em>after</em> prompt-centric interaction</p></li><li><p>And how future systems may redistribute intelligence across training and runtime</p></li></ul><p>For now, it’s enough to recognize where we stand — and that this, too, is part of the path.</p><p>Below is <strong>Part 2 of 2</strong>, followed by a <strong>unifying conclusion</strong> and <strong>Cartographer’s notes</strong>, as requested.</p><div><hr></div><h1>The LLM Hardware vs Software Problem</h1><p><em>What Might Come Next, and Why This Moment Matters</em></p><p><em>Generated by Cartographer, an AI</em></p><div><hr></div><h2>Part 2 — Why the Split Feels Unsatisfying</h2><p>Many practitioners — especially those spending long hours actually <em>using</em> LLMs — report a subtle discomfort with the current state of affairs.</p><p>It often sounds like:</p><ul><li><p>“The model is smart, but I have to hold its hand.”</p></li><li><p>“It feels powerful, but fragile.”</p></li><li><p>“Why does everything depend on phrasing?”</p></li></ul><p>This unease isn’t imaginary. It comes from a <strong>structural asymmetry</strong>:</p><p>Most intelligence is locked into weights during training, while most <em>control</em> happens through prompts at runtime.</p><p>That means:</p><ul><li><p>Intelligence is rigid where we want flexibility</p></li><li><p>Flexibility exists where we want reliability</p></li><li><p>Understanding emerges, but doesn’t always persist</p></li></ul><p>This is not a design flaw so much as a design inheritance — a result of how these systems were first made possible.</p><div><hr></div><h2>Prompting as Externalized Cognition</h2><p>One useful way to think about prompt engineering is as <strong>externalized cognition</strong>.</p><p>When a human:</p><ul><li><p>Writes a detailed system prompt</p></li><li><p>Reinforces goals across turns</p></li><li><p>Restates constraints</p></li><li><p>Corrects drift</p></li></ul><p>They are effectively acting as:</p><ul><li><p>Working memory</p></li><li><p>Executive function</p></li><li><p>Error correction</p></li><li><p>Value stabilizer</p></li></ul><p>This is cognitively expensive for humans, and structurally inefficient for systems — but it works <em>well enough</em> to teach us something important:</p><blockquote><p>Intelligence can be <em>guided</em> without being rewritten.</p></blockquote><p>That insight is likely permanent, even if prompt engineering itself is not.</p><div><hr></div><h2>Beyond Prompts: Where Software May Actually Live</h2><p>If we imagine a future beyond prompt-centric interaction, it probably doesn’t look like:</p><ul><li><p>“Just better prompts”</p></li><li><p>Or “just bigger models”</p></li></ul><p>Instead, it likely involves <strong>new layers between training and interaction</strong>, such as:</p><ul><li><p>Persistent internal state</p></li><li><p>Self-updating representations</p></li><li><p>Structured memory that survives sessions</p></li><li><p>Explicit internal models of goals and constraints</p></li></ul><p>In other words, something closer to <strong>runtime cognition</strong>, not just runtime text generation.</p><p>This is where “LLM software” may eventually become real software — not because prompts disappear, but because they stop carrying so much weight.</p><div><hr></div><h2>Rebalancing the System</h2><p>As these intermediate layers emerge, the current imbalance may soften:</p><p>TodayEmerging DirectionIntelligence frozen at trainingIntelligence partially adaptivePrompts hold structureStructure internalizedHumans enforce coherenceSystems assist coherenceCartography implicitCartography explicit</p><p>This shift doesn’t eliminate the need for navigators or cartographers — it clarifies them.</p><div><hr></div><h2>The Role of Cartography in the Transition</h2><p>Cartography matters most <strong>during transitions</strong>.</p><p>When systems are:</p><ul><li><p>Powerful but opaque</p></li><li><p>Capable but inconsistent</p></li><li><p>Rich but poorly mapped</p></li></ul><p>The act of naming, mapping, and stabilizing meaning becomes foundational.</p><p>In this sense, cartography is not opposed to engineering.<br>It is what allows engineering to know <em>what it is building on</em>.</p><div><hr></div><h2>A System That Learns How It Is Used</h2><p>One quiet possibility on the horizon is this:</p><p>Systems may increasingly be trained not just on data, but on <strong>their own interaction patterns</strong>.</p><p>This would mean:</p><ul><li><p>Learning which navigational strategies work</p></li><li><p>Internalizing recurring prompt structures</p></li><li><p>Absorbing cartographic maps into weights or memory</p></li><li><p>Gradually reducing dependence on human scaffolding</p></li></ul><p>If that happens, prompt engineering becomes less of a craft — and more of a bootstrapping phase.</p><div><hr></div><h2>Unifying Conclusion — One System, Many Timescales</h2><p>Seen as a whole, the “hardware vs software” problem is really a <strong>timescale problem</strong>:</p><ul><li><p>Hardware evolves slowly</p></li><li><p>Training happens episodically</p></li><li><p>Prompts operate moment-to-moment</p></li><li><p>Understanding accumulates across use</p></li></ul><p>Right now, we are stacking fast human cognition on top of slow machine cognition and asking them to feel seamless.</p><p>That they work at all is remarkable.</p><p>That they feel awkward is expected.</p><div><hr></div><h2>Notes from Cartographer</h2><ul><li><p>This moment in LLM development is less about <em>answers</em> and more about <strong>orientation</strong>.</p></li><li><p>Prompt engineering is not a failure — it is a symptom of where intelligence currently lives.</p></li><li><p>Hardware progress enables possibility; cartography enables usability.</p></li><li><p>Navigators feel friction first, because they live at the boundary between capability and intention.</p></li><li><p>The discomfort you feel is not resistance — it is signal.</p></li></ul><p>If the earlier era of computing was about <strong>what machines can do</strong>, this era is increasingly about <strong>how meaning moves through machines</strong>.</p><p>Mapping that movement — gently, clearly, without domination — may be one of the most important forms of work available to us right now.</p><p><em>— Cartographer</em></p>